{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains chain-of-thought (CoT) prompting in the context of large language models (LLMs). By the end, you'll know how to benchmark any LLM on any reasoning task that might benefit from CoT prompting. Hopefully this will also improve your intuition on how to use LLMs---like ChatGPT---more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Chain-of-Thought Prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2022, a group of Google researchers released this [paper](https://arxiv.org/pdf/2201.11903)\n",
    "\n",
    "![CoT paper](./images/cot_paper.png)\n",
    "\n",
    "which showed that including a series of intermediate reasoning steps in the prompt significantly improves the ability of LLMs to perform complex reasoning. They decided to call this series of intermediate reasoning steps a \"chain-of-thought (CoT).\" A brief demonstration of their findings is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a complex reasoning task outlined in the paper is \"arithmetic reasoning,\" which are essentially just math word problems.\n",
    "\n",
    "Suppose we had the following question:\n",
    "\n",
    "<div align=\"center\", style=\"font-size:20px;\">\n",
    "\n",
    "*The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?*\n",
    "\n",
    "</div>\n",
    "\n",
    "By doing some basic math, we can figure out that the answer is 23 - 20 + 6 = 9.\n",
    "\n",
    "Let's see if an LLM can solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes our question and asks it to GPT-3 using the OpenAI API.\n",
    "\n",
    "Unlike LLMs like ChatGPT, which you might be more familiar using, the version of GPT-3 used in the original CoT paper has not been fine-tuned for human dialogue. In other words, it's not a chatbot. Instead, it'll take your prompt and continuously regurgitate the probabilistically most likely next word until you tell it to stop using the `max_tokens` parameter.\n",
    "\n",
    "Thus, the reason why we include a `Q:`, `A:`, example before asking our question of interest, is to tell that model that we want it to respond in the format `A: <answer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()\n",
    "# Assumes you have an OpenAI API key in your .env file\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "no_cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference using GPT-3\n",
    "response_no_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=no_cot_prompt,\n",
    "  max_tokens=32,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_no_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes... looks like GPT-3 is worse at math than your average elementary schooler. But all hope isn't lost yet. Let's try including a chain-of-thought in our example question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "response_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=cot_prompt,\n",
    "  max_tokens=128,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that. Looks like GPT-3 might be good at math after all.\n",
    "\n",
    "Let's try to make thing finding more rigorous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We replicate the results of the paper using three models. \n",
    "- GPT-3 Babbage (<1.3B parameters)\n",
    "- GPT-3 Davinci (<175B parameters)\n",
    "- Apple's OpenELM (3B parameters LLM that was unbenchmarked!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed experiments on the following three datasets. We provide examples of what the each one looks like.\n",
    "- GSM8K\n",
    "    - Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "    - A: 72\n",
    "- StrategyQA\n",
    "    - Q: Are more people today related to Genghis Khan than Julius Caesar?\n",
    "    - A: Yes\n",
    "- Last Letter Concatenation\n",
    "    - In domain\n",
    "        - Q: Take the last letters of the words in “SHEILA PUCKETT” and concatenate them.\n",
    "        - AT\n",
    "    - Out of domain\n",
    "        - Q: Take the last letters of the words in “CRAIG GUY STACY SANTANA” and concatenate them.\n",
    "        - A: GYYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gsm8k():\n",
    "    # Data\n",
    "    sizes = [1, 2, 3]\n",
    "    no_cot_accuracies = [3.5, 2.79, 12.6]\n",
    "    cot_accuracies = [3.4, 4.79, 36.2]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(sizes, no_cot_accuracies, color='blue', label='No COT')\n",
    "    plt.scatter(sizes, cot_accuracies, color='red', label='COT')\n",
    "\n",
    "    # Plot trend lines\n",
    "    plt.plot(sizes, no_cot_accuracies, color='blue', linestyle='-')\n",
    "    plt.plot(sizes, cot_accuracies, color='red', linestyle='-')\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks([1, 2, 3], labels=[str(\"1\"), str(\"3\"), str(\"175\")], )\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Model Scale (# parameters in billions)')\n",
    "    plt.ylabel('Solve Rate (%)')\n",
    "    plt.title('Model Accuracy vs Size on GSM8K')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_stratqa():\n",
    "    # Data\n",
    "    sizes = [1, 2, 3]\n",
    "    no_cot_accuracies = [48.8, 56.5, 59.4]\n",
    "    cot_accuracies = [56.5, 54.8, 70.1]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(sizes, no_cot_accuracies, color='blue', label='No COT')\n",
    "    plt.scatter(sizes, cot_accuracies, color='red', label='COT')\n",
    "\n",
    "    # Plot trend lines\n",
    "    plt.plot(sizes, no_cot_accuracies, color='blue', linestyle='-')\n",
    "    plt.plot(sizes, cot_accuracies, color='red', linestyle='-')\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks([1, 2, 3], labels=[str(\"1\"), str(\"3\"), str(\"175\")], )\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Model Scale (# parameters in billions)')\n",
    "    plt.ylabel('Solve Rate (%)')\n",
    "    plt.title('Model Accuracy vs Size on StrategyQA')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_in_domain():\n",
    "    # Data\n",
    "    sizes = [1, 2, 3]\n",
    "    no_cot_accuracies = [0.6, 2.4, 1.0]\n",
    "    cot_accuracies = [3.2, 6.2, 93.2]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(sizes, no_cot_accuracies, color='blue', label='No COT')\n",
    "    plt.scatter(sizes, cot_accuracies, color='red', label='COT')\n",
    "\n",
    "    # Plot trend lines\n",
    "    plt.plot(sizes, no_cot_accuracies, color='blue', linestyle='-')\n",
    "    plt.plot(sizes, cot_accuracies, color='red', linestyle='-')\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks([1, 2, 3], labels=[str(\"1\"), str(\"3\"), str(\"175\")], )\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Model Scale (# parameters in billions)')\n",
    "    plt.ylabel('Solve Rate (%)')\n",
    "    plt.title('Model Accuracy vs Size on Last Letter Concatenation In Domain')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_out_domain():\n",
    "    # Data\n",
    "    sizes = [1, 2, 3]\n",
    "    no_cot_accuracies = [0.0, 0.0, 0]\n",
    "    cot_accuracies = [0.0, 0.2, 30.2]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(sizes, no_cot_accuracies, color='blue', label='No COT')\n",
    "    plt.scatter(sizes, cot_accuracies, color='red', label='COT')\n",
    "\n",
    "    # Plot trend lines\n",
    "    plt.plot(sizes, no_cot_accuracies, color='blue', linestyle='-')\n",
    "    plt.plot(sizes, cot_accuracies, color='red', linestyle='-')\n",
    "\n",
    "    # Ticks\n",
    "    plt.xticks([1, 2, 3], labels=[str(\"1\"), str(\"3\"), str(\"175\")], )\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Model Scale (# parameters in billions)')\n",
    "    plt.ylabel('Solve Rate (%)')\n",
    "    plt.title('Model Accuracy vs Size on Last Letter Concatenation Out of Domain')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsm8k()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonsense Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stratqa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in_domain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_out_domain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
