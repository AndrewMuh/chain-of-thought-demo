{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains chain-of-thought (CoT) prompting in the context of large language models (LLMs). By the end, you'll know how to benchmark any LLM on any reasoning task that might benefit from CoT prompting. Hopefully this will also improve your intuition on how to use LLMs---like ChatGPT---more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Chain-of-Thought Prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2022, a group of Google researchers released this [paper](https://arxiv.org/pdf/2201.11903)\n",
    "\n",
    "![CoT paper](./images/cot_paper.png)\n",
    "\n",
    "which showed that including a series of intermediate reasoning steps in the prompt significantly improves the ability of LLMs to perform complex reasoning. They decided to call this series of intermediate reasoning steps a \"chain-of-thought (CoT).\" A brief demonstration of their findings is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a complex reasoning task outlined in the paper is \"arithmetic reasoning,\" which are essentially just math word problems.\n",
    "\n",
    "Suppose we had the following question:\n",
    "\n",
    "<div align=\"center\", style=\"font-size:20px;\">\n",
    "\n",
    "*The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?*\n",
    "\n",
    "</div>\n",
    "\n",
    "By doing some basic math, we can figure out that the answer is 23 - 20 + 6 = 9.\n",
    "\n",
    "Let's see if an LLM can solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes our question and asks it to GPT-3 using the OpenAI API.\n",
    "\n",
    "Unlike LLMs like ChatGPT, which you might be more familiar using, the version of GPT-3 used in the original CoT paper has not been fine-tuned for human dialogue. In other words, it's not a chatbot. Instead, it'll take your prompt and continuously regurgitate the probabilistically most likely next word until you tell it to stop using the `max_tokens` parameter.\n",
    "\n",
    "Thus, the reason why we include a `Q:`, `A:`, example before asking our question of interest, is to tell that model that we want it to respond in the format `A: <answer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "from plotting import *\n",
    "\n",
    "load_dotenv()\n",
    "# Assumes you have an OpenAI API key in your .env file\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "no_cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference using GPT-3\n",
    "response_no_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=no_cot_prompt,\n",
    "  max_tokens=32,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_no_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes... looks like GPT-3 is worse at math than your average elementary schooler. But all hope isn't lost yet. Let's try including a chain-of-thought in our example question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "response_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=cot_prompt,\n",
    "  max_tokens=128,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that. Looks like GPT-3 might be good at math after all.\n",
    "\n",
    "Let's try to make thing finding more rigorous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We replicate the results of the paper using three models. \n",
    "- GPT-3 Babbage (<1.3B parameters)\n",
    "- GPT-3 Davinci (<175B parameters)\n",
    "- Apple's OpenELM (3B parameters LLM that was unbenchmarked!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed experiments on the following three datasets and we provide examples of what the each one looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "\n",
    "##### A (No CoT): The answer is 5.\n",
    "\n",
    "##### A (CoT): There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gsm8k()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonsense Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q: Do hamsters provide food for any animals?\n",
    "\n",
    "##### A (No CoT): Yes\n",
    "\n",
    "##### A (CoT): Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stratqa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q: Take the last letters of the words in \"Elon Musk\" and concatenate them.\n",
    "\n",
    "##### A (No CoT): The answer is nk.\n",
    "\n",
    "##### A (CoT): The last letter of \"Elon\" is \"n\". The last letter of \"Musk\" is \"k\". Concatenating them is \"nk\". The answer is nk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in_domain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Take the last letters of the words in “GERALD MCKNIGHT KATRINA CARPENTER” and concatenate them.\n",
    "\n",
    "A: The answer is DIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_out_domain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, it seems like you can significantly improve an LLM's ability to solve reasoning tasks by simply being a bit more thoughtful about your prompt.\n",
    "\n",
    "Furthemermore, one of the lasting impacts of the original CoT paper, is that the GSM-8K dataset is now a standard benchmark used by all tech companies whenever they launch a new LLM. For example, here's Meta's new Llama 3 they released a few weeks ago:\n",
    "\n",
    "![Llama 3 Benchmarks](./images/llama_benchmarks.png)\n",
    "\n",
    "And now you all know how to compute this exact benchmark.\n",
    "\n",
    "Note: our working theory for why Apple decided to leave their OpenELM model unbenchmarked on GSM-8K is because it sucks on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although CoT's ability to improve model performance on reasoning tasks has been undisputed since the original Google research paper came out, there have been many [papers](https://arxiv.org/abs/2305.04388) that question our ability to make and interpretability statements from CoT reasoning. However, we don't have enough time to address these limitations here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
