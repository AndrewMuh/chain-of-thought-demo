{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains chain-of-thought (CoT) prompting in the context of large language models (LLMs). By the end, you'll know how to benchmark any LLM on any reasoning task that might benefit from CoT prompting. Hopefully this will also improve your intuition on how to use LLMs---like ChatGPT---more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Chain-of-Thought Prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2022, a group of Google researchers released this [paper](https://arxiv.org/pdf/2201.11903)\n",
    "\n",
    "![CoT paper](./images/cot_paper.png)\n",
    "\n",
    "which showed that including a series of intermediate reasoning steps in the prompt significantly improves the ability of LLMs to perform complex reasoning. They decided to call this series of intermediate reasoning steps a \"chain-of-thought (CoT).\" A brief demonstration of their findings is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a complex reasoning task outlined in the paper is \"arithmetic reasoning,\" which are essentially just math word problems.\n",
    "\n",
    "Suppose we had the following question:\n",
    "- The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\n",
    "By doing some basic math, we can figure out that the answer is 23 - 20 + 6 = 9.\n",
    "\n",
    "Let's see if an LLM can solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes our question and asks it to GPT-3 using the OpenAI API.\n",
    "\n",
    "Unlike LLMs like ChatGPT, which you might be more familiar using, the version of GPT-3 used in the original CoT paper has not been fine-tuned for human dialogue. In other words, it's not a chatbot. Instead, it'll take your prompt and continuously regurgitate the probabilistically most likely next word until you tell it to stop using the `max_tokens` parameter.\n",
    "\n",
    "Thus, the reason why we include a `Q:`, `A:`, example before asking our question of interest, is to tell that model that we want it to respond in the format `A: <answer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "# Assumes you have an OpenAI API key in your .env file\n",
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "no_cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The answer is 29\n"
     ]
    }
   ],
   "source": [
    "# Run inference using GPT-3\n",
    "response_no_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=no_cot_prompt,\n",
    "  max_tokens=32,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_no_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes... looks like GPT-3 is worse at math than your average elementary schooler. But all hope isn't lost yet. Let's try including a chain-of-thought in our example question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The cafeteria started with 23 apples. They used 20 to make lunch. They bought 6 more. 23 - 20 + 6 = 9. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    "cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "response_cot = client.completions.create(\n",
    "  model=\"davinci-002\",\n",
    "  prompt=cot_prompt,\n",
    "  max_tokens=128,\n",
    "  temperature=0\n",
    ")\n",
    "print(response_cot.choices[0].text.split('\\n', 1)[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you look at that. Looks like GPT-3 might be good at math after all.\n",
    "\n",
    "Let's try to make thing finding more rigorous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
